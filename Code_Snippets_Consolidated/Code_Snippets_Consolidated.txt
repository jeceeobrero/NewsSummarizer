# NEWS SUMMARIZER

This application parses articles from news sources namely, BBC, Daily Mail, and The Guardian, summarizes these articles, and display their information such as title, author, published date, source, url, and its summary. 

## How to run locally
- Type the command in command line.
```
    pip install - r requirements.txt
```
- Navigate to frontend directory and install npm.
```
    npm install
```
- Run npm build.
```
    npm run build
```
- Go back to the base directory where there is manage.py and run this command.
```
    python manage.py runserver
```

# News Article Summarizer

This summarizer's goal is to display the brief and precise content of the news article. It uses GPT-3, an autoregressive language model released in 2020 that uses deep learning to produce human-like text.

## How it Works

1. It defines the API Key for the Open AI.
```
openai.api_key = get_api_key()     
```
2. Generate summary using GPT-3 API with the desired configurations.
```
# Generate summary using GPT-3 API using the configurations
summary = ""
max_tokens = 300
model_engine = "text-davinci-002"
prompt = (f"Please summarize the following text in {max_tokens} tokens:"
            f"{text}")
try:
    completions = openai.Completion.create(
        engine=model_engine, prompt=prompt, max_tokens=max_tokens, n=1, stop=None, temperature=0.5)
    summary = completions.choices[0].text
except Exception as e:
    print("Error: ", e)
```
3. Returns the summary (with no double breaklines) for further procedure.
```
return summary.strip()
```

# Web Scraper

This scraper parses three reputable sources namely, BBC, The Guardian, and Daily Mail. It mainly uses Newspaper3k to acquire information from a browser that is HTML rendered
and uses Beautiful Soup to extract few information from Javascript rendered pages. 

## How it Works

1) Set first the config for the news article parsing. 
```
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0'

config = Config()
config.browser_user_agent = USER_AGENT
config.request_timeout = 10
```
2) Call each news paper source to parse to store their corresponding parsed articles.
```
parsed_bbc = parse_bbc(config)
parsed_guardian = parse_guardian(config)
parsed_daily_mail = parse_daily_mail(config)
```
3) In each parse function, you have to set the news source url and build the newspaper. In order not to parse articles that were parsed before, you set the memoize_articles to be True. This prevents duplicate querying for a given article and can save resources. Note also that article_urls are of type set in order to not store duplicate urls.
```
news_source_url = 'https://www.bbc.com'
article_urls = set()
bbc = newspaper.build(news_source_url, config=config, memoize_articles=False, language='en')
```
3) Initialize the  parsed_articles.
```
 parsed_articles = []
```
4) Parsed for the article information such as title, author, URL, and source. We use Newspaper3k in this one. Newspaper3k is very useful in this one since we don't have to parse thru the news source's HTML which sometimes might change. Thus, this is much safer and secured way to get information.
```
for sub_article in bbc.articles:
    if sub_article.url not in article_urls:
        try:
            article_urls.add(sub_article.url)
            article = Article(sub_article.url, config=config, memoize_articles=True, language='en')
            article.download()
            article.parse()

            # The majority of the article elements are located
            # within the meta data section of the page's
            # navigational structure
            article_title = article.title
            print(article.title)
            article_meta_data = article.meta_data
```
5) Sometimes Newspaper3k can't read some of the information. That's why we have to parse now for other remaining information such as author using Beautiful Soup.
```
            soup = BeautifulSoup(article.html, 'html.parser')
            bbc_dictionary = json.loads("".join(soup.find("script", {"type":"application/ld+json"}).contents))
            # print(bbc_dictionary)

            date_published = [value for (key, value) in bbc_dictionary.items() if key == 'datePublished']
            print(date_published)

            soup_article = BeautifulSoup(article.html, 'lxml')
            article_author = soup_article.find('div', class_='ssrcss-68pt20-Text-TextContributorName e8mq1e96').get_text().split("By ")[1]
            article_author = article_author if article_author else "BBC"
            print(article_author)
            article_text = soup_article.find_all(
                        'article', class_='ssrcss-pv1rh6-ArticleWrapper e1nh2i2l6')
            x = article_text[0].find_all('p')
            
            # Combine all paragraphs 
            list_paragraphs = []
            for p in np.arange(0, len(x)):
                paragraph = x[p].get_text()
                list_paragraphs.append(paragraph)
                article_text = " ".join(list_paragraphs)
```
6) We have extracted the required data which are title, author, content, published date, source, and lastly, URL.
```
            parsed_article = {
                'Title': article_title,
                'Author': article_author,
                'Published Date': date_published,
                'Source': "BBC",
                "URL": sub_article.url,
                "Content": article_text
            }
            print(parsed_article)
            parsed_articles.append(parsed_article)
```
7) If there are any errors in parsing and in extracting data, an error log is displayed. We take not of errors such as ArticleException and MaxRetryError, and if ever there are other errors, we catch them with an Exception.
```
    except ArticleException as e:
        print("Article not found. ")
    except MaxRetryError as e:
        print("Failed to connect to the website. Please check your internet connection.")
    except Exception as e:
        # Handle the exception
        print(f"An error occurred: {e}")
```
8) The parsed articles will be returned as it will used for future processing which is the summary.
```
return parsed_articles
```
9) Combine all parsed articles and return it.
```
    parsed_bbc = parse_bbc(config)
    parsed_guardian = parse_guardian(config)
    parsed_daily_mail = parse_daily_mail(config)
    
    parsed_articles = []
    parsed_articles.extend(parsed_bbc) 
    parsed_articles.extend(parsed_guardian)
    parsed_articles.extend(parsed_daily_mail)
    
    return parsed_articles    
```


## FRONTEND 
# This is the frontend for the News Articles app which displays list of articles and its summaries. This uses ReactJS Framework due to its dynamic features and its helpful libraries.

## Features
1) Displays the list of articles including their title, author, published date, and a sneak peak of its summary in the Home Page sorted by the latest date.
2) Implemented clickable articles that open a detailed view, showing the full summary and a link to the original article.
3) It contains a search bar to filter articles by keywords from its title and summary, and its source.
4) It has pagination to limit the number of articles displayed per page.
5) It is a responsive application that can be accessed with a mobile phone, tab, and desktop.

# How the Page looks like
- First page displays the Home Page with the Home Component. This also renders the News Articles Component, SearchInput Component, Title Component, Badge Component, and 
- In News Articles Component, it displays the News Article Component.
- When you clicked the News Article Component, it will display the Detailed News Article Component.

# Components are 
1) DetailNewsArticle.js - display the clicked article among the articles displayed from the Home Page and it displays information such as title, author, published date, source, summary, and the source link.
```
import React from "react";
import { useLocation } from "react-router-dom";
import { Link } from "react-router-dom";
import Badge from "./Badge";

const DetailNewsArticle = () => {
  const location = useLocation();
  const data = location.state;
  const newsArticle = data ? data : "";

  return (
    <div className="mt-5 h-100 w-50 mx-auto">
      <Link to="/">Go back to Home</Link>
      <h1 className="news__title mt-5">{newsArticle.title}</h1>
      <Badge label={newsArticle.source} />
      <span className="ml-2 news__author">{newsArticle.author}</span>
      <p className="news__published">
        {newsArticle.published_date.substring(0, 10)}
      </p>
      <p className="news__desc text-justify">
        {newsArticle.summarized_content}
      </p>
      <a href={newsArticle.url} target="_blank">Go to its article link.</a>
    </div>
  );
};

export default DetailNewsArticle;
```

2) Home.js - displays the list of articles sorted by latest date with information such as title, author, published date, source, and a sneak peak of its summary. It has pagination where it displays six articles at most. We don't need to use virtualization of data since we already have implemented pagination in the server-side and it only renders 6 pages at most. Virtualization of data is useful whenever large list of data is to be rendered to avoid extensive usage of resources.

When we access the Home Page, it will immediately fire the GET news articles API with the page set initially as one. Whenever we go to another page number, the GET API is called and renders new articles on the page. It also checks if total page is not zero or empty so it won't display a blank page.
```
import React, { useState, useEffect, useCallback } from "react";
import axios from "axios";
import { TransitionGroup, CSSTransition } from "react-transition-group";
import SearchInput from "./SearchInput";
import NewsArticles from "./NewsArticles";
import Heading from "./Heading";
import Pagination from "./Pagination";
import "../App.css";
import load from '../static/images/loading.gif'

const Home = () => {
  const [newsArticles, setNewsArticles] = useState([]);
  const [loading, setLoading] = useState(false);
  const [currentPage, setCurrentPage] = useState(1);
  const [totalPages, setTotalPages] = useState(1);
  const [searchValue, setSearchValue] = useState("");
  const [error, setError] = useState("");

  useEffect(() => {
    // Call the API immediately
    fetchNewsArticles(currentPage);
  }, [currentPage]);

  // Create a GET call with pagination. This is useful whenever there are too many pages.
  // Instead of loading it all at once, just call the page needed to render. 
  const fetchNewsArticles = async (page) => {
    setError(null);
    setLoading(true);
    const res = await axios
      .get(`https://nlpnewsummarizer.azurewebsites.net/api/articles/?page=${page}`)
      .then((response) => {
        console.log(response);
        setTotalPages(response.data.total_pages);
        setNewsArticles(response.data.news_articles);
        setLoading(false);
      })
      .catch((error) => {
        setError(error.message);
        setLoading(false);
      });
  };

  // Change page and use the useCallback hook to memoize the function and avoid unnecessary re-renders.
  const paginate = useCallback((pageNumber) => {
    if (pageNumber >= 1 && pageNumber <= totalPages) {
      setCurrentPage(pageNumber);
    }
  }, [totalPages]);

  return (
    <div className="container mt-5">
      <Heading />
      <SearchInput setSearchValue={setSearchValue} />
      <TransitionGroup>
        {loading ? (
          <h5>Loading <img src={load} className="loading__gif" alt="loading" /></h5>
        ) : error ? (
          <p>{error}</p>
        ) : (
          <>
            <CSSTransition key={currentPage} timeout={100} classNames="fade">
              <NewsArticles
                articles={newsArticles}
                searchVal={searchValue}
                loading={loading}
              />
            </CSSTransition>
            {totalPages > 1 && (
              <Pagination
                currentPage={currentPage}
                totalPages={totalPages}
                paginate={paginate}
              />
            )}
          </>
        )}
      </TransitionGroup>
    </div>
  );
};

export default Home;
```
3) NewsArticles.js - this serves as the container of the NewsArticle.js and contains the logic for filtering the news articles according to the keywords and their source. It displays six articles at most. We don't need to use virtualization of data since we already have implemented pagination in the server-side and it only renders 6 pages at most. Virtualization of data is useful whenever large list of data is to be rendered to avoid extensive usage of resources.
```
import React, { useState, useEffect } from "react";
import NewsArticle from "./NewsArticle";

const NewsArticles = ({ articles = [], searchVal = '', loading = false }) => {
  if (loading) {
    return <h2>Loading...</h2>;
  }
  
  return (
    <div className="all__news">
      {articles.filter((article)=> {
        return (
            searchVal === "" ||
            article.title.toLowerCase().includes(searchVal.toLowerCase()) ||
            article.source.toLowerCase().includes(searchVal.toLowerCase()) ||
            article.summarized_content.toLowerCase().includes(searchVal.toLowerCase())
        );
      }).map((article) => {
        return(
          <NewsArticle newsArticle={article} loading={loading}/>
        );
      })}
    </div>
  );
};

export default NewsArticles;
```
4) NewsArticle.js - this serves as the news article component that is displayed in the home page. This contains the article's title, author, published date, source, and a sneak peak of its summary.
```
import React from "react";
import { Link } from "react-router-dom";
import Badge from "./Badge";
const NewsArticle = ({ newsArticle, loading }) => {
  if (loading) {
    return <h2>Loading...</h2>;
  }

  return (
    <Link to="/newsarticle" state={newsArticle} className="news__article">
      <div className="news">
        <h1 className="news__title ">{newsArticle.title}</h1>
        <Badge label={newsArticle.source} />
        <span className="news__author">{newsArticle.author}</span>
        <p className="news__published">
          {newsArticle.published_date.substring(0, 10)}
        </p>
        <div className="container__news__desc">
          <p className="news__desc">{newsArticle.summarized_content}</p>
        </div>
        <button className="btn btn-link mt-3 text-primary p-0">
          Read More
        </button>
      </div>
    </Link>
  );
};

export default NewsArticle;
```
5) Pagination.js - serves as the display of the pagination ui.
```
import React from 'react';

const Pagination = ({ currentPage, totalPages, paginate }) => {
  const pageNumbers = [...Array(totalPages).keys()].map(i => i + 1);

  return (
    <nav>
     <ul className='pagination my-5 mx-auto float-right'>
        {pageNumbers.map(number => (
          <li key={number} className={`page-item${number === currentPage ? ' active' : ''}`}>
            <a onClick={() => paginate(number)} href={`#`} className='page-link'>
              {number}
            </a>
          </li>
        ))}
      </ul>
    </nav>
  );
};

export default Pagination;
```
6) Heading.js - contains the heading of the app
```
import React from "react";
import Badge from "./Badge";

const Heading = () => {
  return (
    <h2 className="col text-dark mb-3 p-0">
      Get Latest News Articles from {" "}
      <Badge label="BBC" />
      <Badge label="Daily Mail" />
      <Badge label="The Guardian" />
    </h2>
  );
};

export default Heading;
```
7) Badge.js - contains the display of badge or source of the article. This displays specific color for each news source.
```
import React from "react";

const Badge = ({ label }) => {
  const getBadgeColor = (source) => {
    let color;
    if (source === "BBC") {
      color = "badge-primary";
    } else if (source === "Daily Mail") {
      color = "badge-success";
    } else if (source === "The Guardian") {
      color = "badge-warning";
    }

    return color;
  };
  return <span className={"badge mx-1 badge " + getBadgeColor(label)}>{label}</span>;
};

export default Badge;
```
8) SearchInput.js - contains the search input which can filter news articles by title, keywords, and source.
```
import React from "react";

const SearchInput = ({ setSearchValue }) => {
  return (
    <div className="form-group mt-5">
      <input
        className="form-control"
        id="inputdefault"
        type="text"
        placeholder="Search for an article by title, keyword, or news source"
        onChange={(e) => setSearchValue(e.target.value.toLowerCase())}
      />
    </div>
  );
};

export default SearchInput;
```
9) NotFound.js - template for paths that are not specified in routes
```
import React from 'react';

const NotFound = () => {
  return (
    <div className="container mt-5">
      <h1>Page Not Found</h1>
      <p>The page you are looking for does not exist.</p>
    </div>
  );
};

export default NotFound;
```

# Backend

This Backend API displays and parses+summarizes three reputable sources namely, BBC, The Guardian, and Daily Mail. It uses Beautiful Soup for web scraping, GPT-3 for Summarizing and an REST API. This Backend uses Postgresql as its database. It contains News Article model containing title, author, published date, source, summary, and the source link. 

# Files
1) API - this contains the REST API methods such as GET News Articles and POST News Articles.
2) Scraper - parses for news articles from various sources such as BBC, The Guardian, and Daily Mail. It uses Beautiful Soup and lxml as the parser.
3) Summarizer - summarizes and display the brief and precise content of the news article. It uses GPT-3, an autoregressive language model released in 2020 that uses deep learning to produce human-like text.

# Methods
1) GET all news articles sorted according to the latest dates. 
- It can be accessed thru '/api/articles'
2) POST all parsed and summarized articles. 
- Since it involves list of articles, it is best to use atomic transaction in saving news articles which is implemented in this app.
- It also implements caching which will avoid duplication of parsing and summarizing procedures.
- It logs errors when it encountered one.
- It can be accessed thru '/api/articles'

